#+AUTHOR: Simon Hafner
#+TITLE: anlp homework #5
#+OPTIONS: *:nil
#+OPTIONS: toc:nil
#+OPTIONS: toc:nil
#+LATEX_HEADER: \usepackage{savetrees

* Supervised models
** BOW
*** Debate corpus
with cost 0.6

#+BEGIN_LATEX
\begin{verbatim}
--------------------------------------------------------------------------------
Confusion matrix.
Columns give predicted counts. Rows give gold counts.
--------------------------------------------------------------------------------
362	44	48	|	454	negative
86	47	8	|	141	neutral
85	13	102	|	200	positive
----------------------------
533	104	158
negative neutral positive

--------------------------------------------------------------------------------
		64.28	Overall accuracy
--------------------------------------------------------------------------------
P	R	F
67.92	79.74	73.35	negative
45.19	33.33	38.37	neutral
64.56	51.00	56.98	positive
...................................
59.22	54.69	56.23	Average
\end{verbatim}
#+END_LATEX
*** HCR corpus
with cost 0.5
#+BEGIN_LATEX
\begin{verbatim}
--------------------------------------------------------------------------------
Confusion matrix.
Columns give predicted counts. Rows give gold counts.
--------------------------------------------------------------------------------
0	0	1	0	0	0	|	1	
0	0	18	5	4	0	|	27	irrelevant
0	0	347	61	56	0	|	464	negative
0	0	86	63	12	0	|	161	neutral
0	0	102	10	60	0	|	172	positive
0	0	9	1	3	0	|	13	unsure
------------------------------------------------
0	0	563	140	135	0
 irrelevant negative neutral positive unsure

--------------------------------------------------------------------------------
		56.09	Overall accuracy
--------------------------------------------------------------------------------
P	R	F
0.00	0.00	0.00	
0.00	0.00	0.00	irrelevant
61.63	74.78	67.58	negative
45.00	39.13	41.86	neutral
44.44	34.88	39.09	positive
0.00	0.00	0.00	unsure
...................................
25.18	24.80	24.75	Average
\end{verbatim}
#+END_LATEX

** Extended Features
The features can be toggled individually, see the `-f` flag.
*** Tokens
As used in the basic BoW classifier. I don't filter any tokens here,
the ARK POSTagger splits the tweet and prepares the tokens. It also
includes all emoticons.
*** Tags
The tags provided by the ARK Tagger, because it's a low hanging fruit.
*** MPQA
The lexicon used in the previous problem. This helps with words not
previously encountered. There's two versions, one that include
weak/strong subjectivity and one that doesn't. I recommend not
enabling both, but `-x` will do that.
*** Bigrams
Bigrams of Tokens and Tags.
*** Numbers
Used in the debate as follows:

#+BEGIN_QUOTE
Obama +2 For port security #tweetdebate
#+END_QUOTE

Indicates the presence of a positive/negative number.

*** Emoticons?
I considered using features such as emoticons, but they don't really
occur in either the hcr or the debate dataset. They are parsed by
ARK, but not grouped into positive/negative.

*** Twitter-specific stemming?
The corpus does not contain that much emphasized speech, like
`loooove', it is mostly grammatical, unlike the SMS corpus.

** Results
*** Debate
I wasn't able to beat your scores. 
#+BEGIN_LATEX
\begin{verbatim}
--------------------------------------------------------------------------------
Confusion matrix.
Columns give predicted counts. Rows give gold counts.
--------------------------------------------------------------------------------
393	26	35	|	454	negative
93	40	8	|	141	neutral
92	13	95	|	200	positive
----------------------------
578	79	138
negative neutral positive

--------------------------------------------------------------------------------
		66.42	Overall accuracy
--------------------------------------------------------------------------------
P	R	F
67.99	86.56	76.16	negative
50.63	28.37	36.36	neutral
68.84	47.50	56.21	positive
...................................
62.49	54.14	56.25	Average
\end{verbatim}
#+END_LATEX

*** HCR
#+BEGIN_LATEX
\begin{verbatim}
--------------------------------------------------------------------------------
Confusion matrix.
Columns give predicted counts. Rows give gold counts.
--------------------------------------------------------------------------------
0	0	1	0	0	0	|	1	
0	0	21	3	3	0	|	27	irrelevant
0	0	341	69	54	0	|	464	negative
0	0	76	68	17	0	|	161	neutral
0	0	92	16	64	0	|	172	positive
0	0	8	2	3	0	|	13	unsure
------------------------------------------------
0	0	539	158	141	0
 irrelevant negative neutral positive unsure

--------------------------------------------------------------------------------
		56.44	Overall accuracy
--------------------------------------------------------------------------------
P	R	F
0.00	0.00	0.00	
0.00	0.00	0.00	irrelevant
63.27	73.49	68.00	negative
43.04	42.24	42.63	neutral
45.39	37.21	40.89	positive
0.00	0.00	0.00	unsure
...................................
25.28	25.49	25.25	Average
\end{verbatim}
#+END_LATEX

* Stanford
In average, there are more negative than positive tweets in the
training corpora (is this a sign that politics is blaming others?).
The Stanford corpus has more positive samples. As seen in the
confusion matrix, the classifier seldom mistakes a negative for a
positive tweet, but the other way round is very likely.

#+BEGIN_LATEX
\begin{verbatim}
--------------------------------------------------------------------------------
0	0	0	0	|	0	irrelevant
0	53	15	7	|	75	negative
0	11	16	6	|	33	neutral
1	37	22	48	|	108	positive
--------------------------------
1	101	53	61
irrelevant negative neutral positive
\end{verbatim}
#+END_LATEX

* Noisy Labels
With the HCR dataset, the lexicon-based classifier beats the
machine-learning one (cost of 0.7). At leas the ML classifier is
better than the majority one. The same happens with the Debate
dataset. I assume a twitter-specialized stemmer would be of help here,
maybe even a generic Stanford Stemmer.

#+CAPTION: Accuracy
|          | Lexicon | Supervised | Majority |
|----------+---------+------------+----------|
| debate08 |   36.48 |      27.30 |    25.16 |
| hcr      |   36.99 |      31.50 |    20.53 |

#+CAPTION: F-Score
|          | Lexicon | Supervised | Majority |
|----------+---------+------------+----------|
| debate08 |   25.77 |      27.30 |    13.40 |
| hcr      |   15.52 |      14.63 |     5.68 |

Also interesting to observe are the confusion matrices, which indicate
that the ML classifier has a bias towards neutral. I suspect this is
because tweets with emoticons in them tend to correlate with less
formal style, as found in the debate/hcr tweets.

Even adding the other training data (hcr for debate) lifts the ML
classifier over the lexicon-based one. I suspect this combines with
the difference in style mentioned in the previous paragraph.

* Results
Those were taken with

#+BEGIN_QUOTE
-f tokens tags tokensntags numbers MPQAcomplex tokenbigrams tagbigrams
#+END_QUOTE

#+CAPTION: Outputs for hcr dev set
| Model      | Training | Cost | Overall | Negative | Neutral | Positive | Average |
|------------+----------+------+---------+----------+---------+----------+---------|
| Lexicon    |          |      |   39.86 |    51.32 |   29.25 |    34.15 |   19.12 |
| L2R_LR (B) | hcr      |  0.6 |   55.97 |    67.51 |   41.72 |    39.23 |   24.74 |
| L2R_LR (E) | hcr      |  0.6 |   56.44 |    67.93 |   42.63 |    41.03 |   25.26 |

#+CAPTION: Outputs for debate dev set
| Model      | Training | Cost | Overall | Negative | Neutral | Positive | Average |
|------------+----------+------+---------+----------+---------+----------+---------|
| Lexicon    |          |      |   41.64 |    51.56 |   27.27 |    38.22 |   39.02 |
| L2R_LR (B) | debate08 | 0.6  |   64.28 |    73.35 |   38.37 |    56.98 |   56.23 |
| L2R_LR (E) | debate08 | 0.6  |   66.42 |    76.16 |   36.36 |    56.21 |   56.25 |

The average for the extended is so bad because it has one tweet tagged
as `irrelevant'. The basic one has `irrelevant' and `unsure'.
#+CAPTION: Outputs for stanford dev set
| Model      | Training     | Cost | Overall | Negative | Neutral | Positive | Average |
|------------+--------------+------+---------+----------+---------+----------+---------|
| Lexicon    |              |      |   57.87 |    61.76 |   40.37 |    65.24 |   55.79 |
| L2R_LR (B) | debate08/hcr |  0.6 |   54.17 |    54.88 |   34.29 |    61.54 |   30.14 |
| L2R_LR (E) | debate08/hcr |  0.6 |   54.17 |    60.23 |   37.21 |    56.80 |   38.56 |


** Detailed analysis
Some labels are also questionable, this tweet was labeled as positive
(from the training set).
#+BEGIN_QUOTE
mccain as president= torture #current #debate08
#+END_QUOTE

*** Positive mistaken as Negative
This is correctly negative, wrong gold label.
#+BEGIN_QUOTE
Okay, this is a done deal. Anybody knows a good spot to watch the end of the world, before things go back to normal tomorrow? #hcr
#+END_QUOTE

I'm not sure about this one, this seems negative to me.
#+BEGIN_QUOTE
Why don't the people against #HCR just confess to the fact that they are in love with the idea of genocide?
#+END_QUOTE

Vision is a positive word, I wonder why it didn't order this one correctly.
#+BEGIN_QUOTE
Roosevelt had the vision, Clinton attempted it, Obama will execute it #hcr
#+END_QUOTE

Looks rather negative to me, I'd sort this as `negative'.
#+BEGIN_QUOTE
Our Beloved President Ronald Reagen Is Rolling In his Grave..#teaparty #tcot #gop #hcr
#+END_QUOTE

Clearly positive, the classifier should have gotten that one.
#+BEGIN_QUOTE
Excellent speech by Connie Mack of Florida!!!! ''The American people deserve to be listened to''!!! #p2 #tcot #hcr
#+END_QUOTE

This one as well.
#+BEGIN_QUOTE
Gergen: Democrats have been dreaming of this for 60yrs. Obama now has a legacy. First president to achieve near universal coverage. #p2 #hcr
#+END_QUOTE

Sarcasm. Should be labeled as negative IMO.
#+BEGIN_QUOTE
Um... a bill that supports the for-profit status quo without even a public option to compete is socialized medicine? Get a grip #tcot #hcr
#+END_QUOTE

A positive one. *kicks classifier*
#+BEGIN_QUOTE
After this passes, Pelosi goes down as one of the top 3 most powerful Speakers in American history. Stimulus, Cap & Trade, 2 #hcr in 1 year.
#+END_QUOTE

Another instance of beautiful sarcasm.
#+BEGIN_QUOTE
i mourn for the future of america when i see how miserable and oppressed the swedes, japanese, and germans are. wait... #hcr
#+END_QUOTE

I think the `not' plays a role here.
#+BEGIN_QUOTE
Dear President Obama, Thank you for not giving up. #hcr
#+END_QUOTE

A lot of neutral, then a bit positive. Maybe something in the neutral
part tripped the classifier.
#+BEGIN_QUOTE
Guys, just got back from my accountant's who is a small business owner. He supports #HCR #p2
#+END_QUOTE

*** Negative mistaken as Positive
A bunch of positive indicators, like `feel' and `good'. And the
second sentence is the indicator, not so easy to grab that. 
#+BEGIN_QUOTE
Don't vote ''yes'' because you feel it's good for America. Vote ''yes'' b/c you were bribed or lied to, having not read the bill yourself. #hcr
#+END_QUOTE

A bit of sarcasm mixed with just a few negative words. Very hard for
a classifier as simple as this one.
#+BEGIN_QUOTE
A further loss of liberty is upon us as the nimrods in Washington vote on #hcr. Enjoy your freedom while you can...
#+END_QUOTE

Another last-sentence-turner.
#+BEGIN_QUOTE
Think of #hcr as a desk from Ikea. At first it's nice and easy to assemble, but then it scratches! #shelleymoorecapitoanalogies
#+END_QUOTE

Only positive words, combined negative. This would require a
fixed-clause lexicon.
#+BEGIN_QUOTE
Its time to clean HOUSE! #hcr #tcot
#+END_QUOTE

Strange. I can't explain this one.
#+BEGIN_QUOTE
Shut up John Boehner. #hcr
#+END_QUOTE

I see strongly negative words in here. Should have gotten this one.
#+BEGIN_QUOTE
RT @mcjusa: EVERY SINGLE ONE OF THESE SELF RIGHTEOUS WHORES EXEMPTED THEMSELVES FROM THIS WONDERFUL #HCR LAST NITE Heloooooooo #p2#hcrWAKEUP
#+END_QUOTE

Maybe a stemmer for `lies` would help here.
#+BEGIN_QUOTE
Jeb Hebsarling just crammed 20 lies into 2 minutes. #hcr
#+END_QUOTE

Oh, reference to cultural signs. That's hard to resolve. Why doesn't
anyone like the French around here? :-(
#+BEGIN_QUOTE
RT @Dataaide:  @Howardman1: The French called &want the statue of liberty back!! @foxandfriends @txjogger12 @grettawire #hcr // LOL
#+END_QUOTE

Positive first sentence and the second one isn't clearly negative either.
#+BEGIN_QUOTE
Rep. Rahall's phone lines are now working! - Call him and tell him what you think about his YES vote on #HCR. | 202-225-3452 #WV03
#+END_QUOTE

*** Negative mistaken as neutral
All caps as feature would have helped on this one, at least to put it
either to positive or negative.
#+BEGIN_QUOTE
RECONCILIATION ``FIX'' BLOCKED BY LAW @kendrickbmeek @gregorymeeks #tweetcongress #hcr http://bit.ly/CannotReconcile
#+END_QUOTE


** Additional Strategies
I assume a stemmer, preferably designed for Twitter, would help here.
Also a `not' inverting the statement should be its own feature
somehow. A sarcasm detector is damn hard to do, but would help as
well.
